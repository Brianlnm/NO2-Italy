{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\leeno\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering + Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag(df, num_lags):\n",
    "    df['year'] = pd.to_datetime(df['Date']).dt.year\n",
    "    df['month'] = pd.to_datetime(df['Date']).dt.month\n",
    "    df['day'] = pd.to_datetime(df['Date']).dt.day\n",
    "    df['Date'] = pd.to_datetime(df['Date'])    \n",
    "    df.set_index('Date', inplace=True)\n",
    "    df.sort_values(by=['ID', 'Date'], inplace=True)\n",
    "    for i in range(1, num_lags+1):\n",
    "        df[f'lag{i}'] = df.groupby('ID')['GT_NO2'].shift(i)\n",
    "\n",
    "    return df\n",
    "\n",
    "def full_df(df):\n",
    "    df = df.copy().reset_index()\n",
    "    df = df.drop(['ID', 'ID_Zindi', 'Date', 'NO2_trop'],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leeno\\AppData\\Local\\Temp\\ipykernel_20696\\3738387936.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['year'] = pd.to_datetime(df['Date']).dt.year\n",
      "C:\\Users\\leeno\\AppData\\Local\\Temp\\ipykernel_20696\\3738387936.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['month'] = pd.to_datetime(df['Date']).dt.month\n",
      "C:\\Users\\leeno\\AppData\\Local\\Temp\\ipykernel_20696\\3738387936.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['day'] = pd.to_datetime(df['Date']).dt.day\n",
      "C:\\Users\\leeno\\AppData\\Local\\Temp\\ipykernel_20696\\3738387936.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Date'] = pd.to_datetime(df['Date'])\n"
     ]
    }
   ],
   "source": [
    "trained = create_lag(train, 15)\n",
    "trained = full_df(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training + validation\n",
    "\n",
    "* Min RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts = TimeSeriesSplit(max_train_size=5000)\n",
    "\n",
    "sub_set = trained.dropna()\n",
    "\n",
    "x = sub_set.drop('GT_NO2', axis=1)\n",
    "y = sub_set['GT_NO2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 15\n",
    "\n",
    "def create_sequences(data, labels, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x_seq = data[i:i+seq_length]\n",
    "        y_seq = labels.iloc[i+seq_length]\n",
    "        xs.append(x_seq)\n",
    "        ys.append(y_seq)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(x)\n",
    "X, y = create_sequences(scaled_features, y, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\leeno\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\leeno\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 15, 128)           79872     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131905 (515.25 KB)\n",
      "Trainable params: 131905 (515.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def my_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation='relu', input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))  # Dropout layer added\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))  # Dropout layer added\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "input_shape = (seq_length, X.shape[2])\n",
    "model = my_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:From c:\\Users\\leeno\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "141/141 [==============================] - 6s 24ms/step - loss: 207.0643 - val_loss: 102.5765\n",
      "Epoch 2/200\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 134.3541 - val_loss: 99.1838\n",
      "Epoch 3/200\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 126.7472 - val_loss: 106.6983\n",
      "Epoch 4/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 124.2510 - val_loss: 98.4497\n",
      "Epoch 5/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 125.4161 - val_loss: 103.9925\n",
      "Epoch 6/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 120.9428 - val_loss: 96.4367\n",
      "Epoch 7/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 122.1383 - val_loss: 95.5044\n",
      "Epoch 8/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 118.5371 - val_loss: 98.7461\n",
      "Epoch 9/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 117.1025 - val_loss: 96.8504\n",
      "Epoch 10/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 118.6964 - val_loss: 101.4831\n",
      "Epoch 11/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 113.7788 - val_loss: 100.0951\n",
      "Epoch 12/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 113.2806 - val_loss: 95.9737\n",
      "Epoch 13/200\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 110.9998 - val_loss: 93.5431\n",
      "Epoch 14/200\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 117.0752 - val_loss: 94.9156\n",
      "Epoch 15/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 110.7105 - val_loss: 90.1020\n",
      "Epoch 16/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 107.0972 - val_loss: 96.9478\n",
      "Epoch 17/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 104.0822 - val_loss: 95.3394\n",
      "Epoch 18/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 101.3732 - val_loss: 87.5982\n",
      "Epoch 19/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 103.9552 - val_loss: 98.6544\n",
      "Epoch 20/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 99.6524 - val_loss: 90.5728\n",
      "Epoch 21/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 97.6119 - val_loss: 85.9513\n",
      "Epoch 22/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 96.9828 - val_loss: 87.9046\n",
      "Epoch 23/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 92.6582 - val_loss: 83.0245\n",
      "Epoch 24/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 91.6252 - val_loss: 85.4909\n",
      "Epoch 25/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 91.9668 - val_loss: 93.5301\n",
      "Epoch 26/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 90.1615 - val_loss: 83.0355\n",
      "Epoch 27/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 88.7544 - val_loss: 83.6045\n",
      "Epoch 28/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 89.9302 - val_loss: 85.2355\n",
      "Epoch 29/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 87.8243 - val_loss: 99.6822\n",
      "Epoch 30/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 87.9348 - val_loss: 84.7479\n",
      "Epoch 31/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 85.2449 - val_loss: 79.4952\n",
      "Epoch 32/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 84.4176 - val_loss: 82.8785\n",
      "Epoch 33/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 83.5046 - val_loss: 86.0080\n",
      "Epoch 34/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 81.4754 - val_loss: 82.4999\n",
      "Epoch 35/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 83.9095 - val_loss: 82.2875\n",
      "Epoch 36/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 82.3876 - val_loss: 78.9641\n",
      "Epoch 37/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 81.0564 - val_loss: 79.2426\n",
      "Epoch 38/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 81.1052 - val_loss: 79.2256\n",
      "Epoch 39/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 81.0470 - val_loss: 80.4560\n",
      "Epoch 40/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 79.1787 - val_loss: 82.9142\n",
      "Epoch 41/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 77.6264 - val_loss: 82.4928\n",
      "Epoch 42/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 79.6085 - val_loss: 87.5746\n",
      "Epoch 43/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 76.8361 - val_loss: 87.5841\n",
      "Epoch 44/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 78.1375 - val_loss: 83.5906\n",
      "Epoch 45/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 76.4628 - val_loss: 82.8242\n",
      "Epoch 46/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 75.4237 - val_loss: 95.4392\n",
      "Fold 1 - Training Loss: 75.4237, Validation Loss: 95.4392\n",
      "165/165 [==============================] - 2s 8ms/step\n",
      "Fold 1 - Test Loss (MSE): 110.9634\n",
      "Epoch 1/200\n",
      "141/141 [==============================] - 5s 21ms/step - loss: 217.7235 - val_loss: 145.4078\n",
      "Epoch 2/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 132.1432 - val_loss: 143.1814\n",
      "Epoch 3/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 133.4207 - val_loss: 141.6154\n",
      "Epoch 4/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 129.4105 - val_loss: 144.4091\n",
      "Epoch 5/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 128.1203 - val_loss: 142.9910\n",
      "Epoch 6/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 127.9716 - val_loss: 143.9152\n",
      "Epoch 7/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 124.6773 - val_loss: 140.5033\n",
      "Epoch 8/200\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 127.6233 - val_loss: 140.9525\n",
      "Epoch 9/200\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 125.3633 - val_loss: 148.2921\n",
      "Epoch 10/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 123.4544 - val_loss: 148.4119\n",
      "Epoch 11/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 120.6705 - val_loss: 143.5514\n",
      "Epoch 12/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 123.2468 - val_loss: 142.0080\n",
      "Epoch 13/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 120.7811 - val_loss: 144.0928\n",
      "Epoch 14/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 120.9454 - val_loss: 139.4267\n",
      "Epoch 15/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 124.5890 - val_loss: 141.4487\n",
      "Epoch 16/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 118.8759 - val_loss: 140.5010\n",
      "Epoch 17/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 118.7039 - val_loss: 149.8836\n",
      "Epoch 18/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 115.7368 - val_loss: 142.8407\n",
      "Epoch 19/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 113.1810 - val_loss: 155.2559\n",
      "Epoch 20/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 113.9627 - val_loss: 148.0367\n",
      "Epoch 21/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 110.9178 - val_loss: 146.1648\n",
      "Epoch 22/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 111.3982 - val_loss: 145.8997\n",
      "Epoch 23/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 113.3657 - val_loss: 141.8576\n",
      "Epoch 24/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 106.6904 - val_loss: 149.9829\n",
      "Fold 2 - Training Loss: 106.6904, Validation Loss: 149.9829\n",
      "165/165 [==============================] - 2s 8ms/step\n",
      "Fold 2 - Test Loss (MSE): 331.7631\n",
      "Epoch 1/200\n",
      "141/141 [==============================] - 5s 21ms/step - loss: 432.0486 - val_loss: 413.9314\n",
      "Epoch 2/200\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 317.3770 - val_loss: 432.7580\n",
      "Epoch 3/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 310.2802 - val_loss: 425.9686\n",
      "Epoch 4/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 305.4908 - val_loss: 448.4157\n",
      "Epoch 5/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 304.1178 - val_loss: 432.5775\n",
      "Epoch 6/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 304.1371 - val_loss: 464.4704\n",
      "Epoch 7/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 304.8478 - val_loss: 480.7246\n",
      "Epoch 8/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 300.2614 - val_loss: 427.2581\n",
      "Epoch 9/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 301.4313 - val_loss: 413.8300\n",
      "Epoch 10/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 296.4893 - val_loss: 423.3762\n",
      "Epoch 11/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 290.6989 - val_loss: 399.0009\n",
      "Epoch 12/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 290.9225 - val_loss: 400.9163\n",
      "Epoch 13/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 283.4264 - val_loss: 419.5194\n",
      "Epoch 14/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 279.7072 - val_loss: 466.5209\n",
      "Epoch 15/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 272.3955 - val_loss: 355.4370\n",
      "Epoch 16/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 269.1867 - val_loss: 405.0301\n",
      "Epoch 17/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 264.3822 - val_loss: 384.3566\n",
      "Epoch 18/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 258.0437 - val_loss: 391.6622\n",
      "Epoch 19/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 259.5702 - val_loss: 379.0151\n",
      "Epoch 20/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 257.7894 - val_loss: 356.6091\n",
      "Epoch 21/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 251.8609 - val_loss: 349.3737\n",
      "Epoch 22/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 238.4256 - val_loss: 332.6888\n",
      "Epoch 23/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 242.3289 - val_loss: 363.1917\n",
      "Epoch 24/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 248.9942 - val_loss: 385.3250\n",
      "Epoch 25/200\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 247.9976 - val_loss: 347.9683\n",
      "Epoch 26/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 239.2619 - val_loss: 375.5211\n",
      "Epoch 27/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 230.6122 - val_loss: 379.6626\n",
      "Epoch 28/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 234.5255 - val_loss: 404.2173\n",
      "Epoch 29/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 232.2817 - val_loss: 371.8594\n",
      "Epoch 30/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 231.6745 - val_loss: 337.0472\n",
      "Epoch 31/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 233.0067 - val_loss: 335.5780\n",
      "Epoch 32/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 224.9478 - val_loss: 344.7841\n",
      "Fold 3 - Training Loss: 224.9478, Validation Loss: 344.7841\n",
      "165/165 [==============================] - 2s 8ms/step\n",
      "Fold 3 - Test Loss (MSE): 193.2116\n",
      "Epoch 1/200\n",
      "141/141 [==============================] - 6s 21ms/step - loss: 318.0700 - val_loss: 163.2263\n",
      "Epoch 2/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 229.8179 - val_loss: 172.6235\n",
      "Epoch 3/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 227.8299 - val_loss: 159.9924\n",
      "Epoch 4/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 223.7329 - val_loss: 158.9748\n",
      "Epoch 5/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 215.7260 - val_loss: 172.7958\n",
      "Epoch 6/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 214.0875 - val_loss: 156.2079\n",
      "Epoch 7/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 209.4996 - val_loss: 169.3551\n",
      "Epoch 8/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 212.1189 - val_loss: 155.3992\n",
      "Epoch 9/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 197.7687 - val_loss: 149.4401\n",
      "Epoch 10/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 195.2888 - val_loss: 142.9004\n",
      "Epoch 11/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 191.1794 - val_loss: 143.2280\n",
      "Epoch 12/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 190.6000 - val_loss: 140.1475\n",
      "Epoch 13/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 192.4021 - val_loss: 140.6168\n",
      "Epoch 14/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 185.9004 - val_loss: 143.1994\n",
      "Epoch 15/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 185.1012 - val_loss: 144.4286\n",
      "Epoch 16/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 187.8146 - val_loss: 133.7434\n",
      "Epoch 17/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 181.0487 - val_loss: 145.9236\n",
      "Epoch 18/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 177.6146 - val_loss: 134.2840\n",
      "Epoch 19/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 176.6291 - val_loss: 133.0911\n",
      "Epoch 20/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 173.5272 - val_loss: 131.0780\n",
      "Epoch 21/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 172.9676 - val_loss: 129.5295\n",
      "Epoch 22/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 173.3099 - val_loss: 139.4487\n",
      "Epoch 23/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 171.7335 - val_loss: 131.3694\n",
      "Epoch 24/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 168.6513 - val_loss: 125.4898\n",
      "Epoch 25/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 165.1557 - val_loss: 129.3266\n",
      "Epoch 26/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 164.7314 - val_loss: 124.5452\n",
      "Epoch 27/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 157.4953 - val_loss: 123.0593\n",
      "Epoch 28/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 163.9177 - val_loss: 130.6982\n",
      "Epoch 29/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 157.2679 - val_loss: 118.2969\n",
      "Epoch 30/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 157.8925 - val_loss: 123.4347\n",
      "Epoch 31/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 154.3242 - val_loss: 121.0410\n",
      "Epoch 32/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 151.8321 - val_loss: 117.4851\n",
      "Epoch 33/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 148.8167 - val_loss: 125.3402\n",
      "Epoch 34/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 149.3824 - val_loss: 117.4285\n",
      "Epoch 35/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 148.9780 - val_loss: 123.6485\n",
      "Epoch 36/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 144.0159 - val_loss: 116.6346\n",
      "Epoch 37/200\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 143.7160 - val_loss: 115.8773\n",
      "Epoch 38/200\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 140.6759 - val_loss: 116.4331\n",
      "Epoch 39/200\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 140.5823 - val_loss: 127.5568\n",
      "Epoch 40/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 140.1822 - val_loss: 125.9181\n",
      "Epoch 41/200\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 134.7308 - val_loss: 119.7987\n",
      "Epoch 42/200\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 136.3494 - val_loss: 122.1068\n",
      "Epoch 43/200\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 133.9535 - val_loss: 118.8099\n",
      "Epoch 44/200\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 132.5273 - val_loss: 118.4737\n",
      "Epoch 45/200\n",
      "141/141 [==============================] - 6s 41ms/step - loss: 136.8429 - val_loss: 131.9789\n",
      "Epoch 46/200\n",
      "141/141 [==============================] - 4s 26ms/step - loss: 129.5121 - val_loss: 112.0548\n",
      "Epoch 47/200\n",
      "141/141 [==============================] - 4s 27ms/step - loss: 131.1082 - val_loss: 118.4074\n",
      "Epoch 48/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 127.7791 - val_loss: 121.1420\n",
      "Epoch 49/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 126.2800 - val_loss: 127.6731\n",
      "Epoch 50/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 128.3291 - val_loss: 120.3157\n",
      "Epoch 51/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 123.0344 - val_loss: 118.4346\n",
      "Epoch 52/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 117.8932 - val_loss: 121.3559\n",
      "Epoch 53/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 119.0200 - val_loss: 111.0820\n",
      "Epoch 54/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 117.5786 - val_loss: 119.3702\n",
      "Epoch 55/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 116.1458 - val_loss: 112.6001\n",
      "Epoch 56/200\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 116.5840 - val_loss: 123.9815\n",
      "Epoch 57/200\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 113.8170 - val_loss: 114.6218\n",
      "Epoch 58/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 117.7223 - val_loss: 112.9439\n",
      "Epoch 59/200\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 112.3643 - val_loss: 110.7809\n",
      "Epoch 60/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 113.6970 - val_loss: 133.2852\n",
      "Epoch 61/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 108.6927 - val_loss: 110.7310\n",
      "Epoch 62/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 108.8485 - val_loss: 120.8644\n",
      "Epoch 63/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 109.0559 - val_loss: 122.2553\n",
      "Epoch 64/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 105.5271 - val_loss: 113.7588\n",
      "Epoch 65/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 102.4939 - val_loss: 109.7997\n",
      "Epoch 66/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 103.6702 - val_loss: 124.8748\n",
      "Epoch 67/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 100.5624 - val_loss: 114.4279\n",
      "Epoch 68/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 97.9139 - val_loss: 105.6524\n",
      "Epoch 69/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 97.1123 - val_loss: 115.0199\n",
      "Epoch 70/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 96.6247 - val_loss: 115.7293\n",
      "Epoch 71/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 92.9381 - val_loss: 117.5378\n",
      "Epoch 72/200\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 96.2461 - val_loss: 113.8338\n",
      "Epoch 73/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 89.4721 - val_loss: 121.3948\n",
      "Epoch 74/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 92.5311 - val_loss: 118.0510\n",
      "Epoch 75/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 88.1453 - val_loss: 113.7164\n",
      "Epoch 76/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 88.4527 - val_loss: 110.8083\n",
      "Epoch 77/200\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 87.4083 - val_loss: 123.1993\n",
      "Epoch 78/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 85.6583 - val_loss: 115.5275\n",
      "Fold 4 - Training Loss: 85.6583, Validation Loss: 115.5275\n",
      "165/165 [==============================] - 2s 7ms/step\n",
      "Fold 4 - Test Loss (MSE): 126.5486\n",
      "Epoch 1/200\n",
      "141/141 [==============================] - 6s 19ms/step - loss: 209.3949 - val_loss: 299.8474\n",
      "Epoch 2/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 142.2415 - val_loss: 350.7512\n",
      "Epoch 3/200\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 138.8273 - val_loss: 273.8505\n",
      "Epoch 4/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 138.5279 - val_loss: 286.6454\n",
      "Epoch 5/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 134.3533 - val_loss: 410.1695\n",
      "Epoch 6/200\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 137.2956 - val_loss: 398.0110\n",
      "Epoch 7/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 137.1338 - val_loss: 445.3276\n",
      "Epoch 8/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 136.1414 - val_loss: 342.0820\n",
      "Epoch 9/200\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 132.4389 - val_loss: 392.8089\n",
      "Epoch 10/200\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 129.3230 - val_loss: 366.8290\n",
      "Epoch 11/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 129.3762 - val_loss: 267.9536\n",
      "Epoch 12/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 122.2939 - val_loss: 452.5432\n",
      "Epoch 13/200\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 114.6998 - val_loss: 324.3973\n",
      "Epoch 14/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 116.5297 - val_loss: 397.5399\n",
      "Epoch 15/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 111.5149 - val_loss: 280.5068\n",
      "Epoch 16/200\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 120.0583 - val_loss: 621.1035\n",
      "Epoch 17/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 110.2804 - val_loss: 537.8271\n",
      "Epoch 18/200\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 105.9722 - val_loss: 341.0807\n",
      "Epoch 19/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 106.2010 - val_loss: 502.7816\n",
      "Epoch 20/200\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 102.6935 - val_loss: 359.3469\n",
      "Epoch 21/200\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 104.5666 - val_loss: 307.5233\n",
      "Fold 5 - Training Loss: 104.5666, Validation Loss: 307.5233\n",
      "165/165 [==============================] - 2s 7ms/step\n",
      "Fold 5 - Test Loss (MSE): 150.1086\n"
     ]
    }
   ],
   "source": [
    "# Initialize TimeSeriesSplit with max_train_size\n",
    "tscv = TimeSeriesSplit(max_train_size=5000)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Training and evaluation loop\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = my_model(input_shape)\n",
    "    \n",
    "    # Train the model with early stopping and validation data\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        validation_split=0.1,  # Use 10% of training data for validation\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Print training loss and validation loss from the last epoch\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f'Fold {fold} - Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_loss = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Fold {fold} - Test Loss (MSE): {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history.history['loss'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
